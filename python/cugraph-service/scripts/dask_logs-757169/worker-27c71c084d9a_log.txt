RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --enable-tcp-over-ucx
                --enable-nvlink
                --disable-infiniband
                --disable-rdmacm
                --rmm-pool-size=12G
                --rmm-maximum-pool-size=12G
                --local-directory=/tmp/
                --scheduler-file=/work/cugraph/python/cugraph-service/scripts/dask-scheduler.json
                --memory-limit=auto
                --device-memory-limit=auto
                --enable-jit-unspill
                "
2022-12-10 20:37:31,663 - distributed.nanny - INFO -         Start Nanny at: 'ucx://172.17.0.2:36193'
2022-12-10 20:37:31,689 - distributed.nanny - INFO -         Start Nanny at: 'ucx://172.17.0.2:37089'
2022-12-10 20:37:32,768 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-10 20:37:32,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-12-10 20:37:32,777 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-10 20:37:32,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-12-10 20:38:21,668 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2022-12-10 20:38:21,670 - distributed.worker - INFO -       Start worker at:     ucx://172.17.0.2:43155
2022-12-10 20:38:21,670 - distributed.worker - INFO -          Listening to:     ucx://172.17.0.2:43155
2022-12-10 20:38:21,670 - distributed.worker - INFO -          dashboard at:           172.17.0.2:37553
2022-12-10 20:38:21,670 - distributed.worker - INFO - Waiting to connect to:      ucx://172.17.0.2:8786
2022-12-10 20:38:21,670 - distributed.worker - INFO - -------------------------------------------------
2022-12-10 20:38:21,670 - distributed.worker - INFO -               Threads:                          1
2022-12-10 20:38:21,670 - distributed.worker - INFO -                Memory:                  31.31 GiB
2022-12-10 20:38:21,670 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9dfuqxyt
2022-12-10 20:38:21,671 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72c3bda7-9367-4648-9a0a-8f57d17f6bd7
2022-12-10 20:38:21,676 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2022-12-10 20:38:21,679 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e7d7cc6-2f65-4061-9cdf-bf8de19e99c6
2022-12-10 20:38:21,679 - distributed.worker - INFO - Starting Worker plugin PreImport-28e8899a-0ee9-4b31-9748-b45ce1b21687
2022-12-10 20:38:21,679 - distributed.worker - INFO - -------------------------------------------------
2022-12-10 20:38:21,679 - distributed.worker - INFO -       Start worker at:     ucx://172.17.0.2:34393
2022-12-10 20:38:21,679 - distributed.worker - INFO -          Listening to:     ucx://172.17.0.2:34393
2022-12-10 20:38:21,680 - distributed.worker - INFO -          dashboard at:           172.17.0.2:37067
2022-12-10 20:38:21,680 - distributed.worker - INFO - Waiting to connect to:      ucx://172.17.0.2:8786
2022-12-10 20:38:21,680 - distributed.worker - INFO - -------------------------------------------------
2022-12-10 20:38:21,680 - distributed.worker - INFO -               Threads:                          1
2022-12-10 20:38:21,680 - distributed.worker - INFO -                Memory:                  31.31 GiB
2022-12-10 20:38:21,680 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k2qjhc_z
2022-12-10 20:38:21,680 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c691ac2-d924-4a96-b871-e5a875398e50
2022-12-10 20:38:21,680 - distributed.worker - INFO - Starting Worker plugin PreImport-c306cc59-5f17-43e9-9deb-4046c7b5c573
2022-12-10 20:38:21,680 - distributed.worker - INFO - Starting Worker plugin RMMSetup-183e0abe-e311-4f14-9fc5-346b410c4d51
2022-12-10 20:38:21,688 - distributed.worker - INFO - -------------------------------------------------
2022-12-10 20:38:21,769 - distributed.worker - INFO -         Registered to:      ucx://172.17.0.2:8786
2022-12-10 20:38:21,769 - distributed.worker - INFO - -------------------------------------------------
2022-12-10 20:38:21,770 - distributed.core - INFO - Starting established connection to ucx://172.17.0.2:8786
2022-12-10 20:38:21,771 - distributed.worker - INFO -         Registered to:      ucx://172.17.0.2:8786
2022-12-10 20:38:21,771 - distributed.worker - INFO - -------------------------------------------------
2022-12-10 20:38:21,772 - distributed.core - INFO - Starting established connection to ucx://172.17.0.2:8786
2022-12-10 20:38:26,224 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2022-12-10 20:38:26,225 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2022-12-10 20:38:26,355 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2022-12-10 20:38:26,355 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2022-12-10 20:38:31,108 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2022-12-10 20:38:31,116 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2022-12-10 20:38:33,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2022-12-10 20:38:33,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2022-12-10 20:40:31,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2022-12-10 20:40:38,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2022-12-12 15:26:53,986 - distributed.nanny - INFO - Closing Nanny at 'ucx://172.17.0.2:36193'. Reason: nanny-close
2022-12-12 15:26:53,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2022-12-12 15:26:53,987 - distributed.nanny - INFO - Closing Nanny at 'ucx://172.17.0.2:37089'. Reason: nanny-close
2022-12-12 15:26:53,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2022-12-12 15:26:53,987 - distributed.core - INFO - Connection to ucx://172.17.0.2:8786 has been closed.
2022-12-12 15:26:53,987 - distributed.worker - INFO - Stopping worker at ucx://172.17.0.2:43155. Reason: worker-handle-scheduler-connection-broken
2022-12-12 15:26:53,987 - distributed.core - INFO - Connection to ucx://172.17.0.2:8786 has been closed.
2022-12-12 15:26:53,988 - distributed.worker - INFO - Stopping worker at ucx://172.17.0.2:34393. Reason: worker-handle-scheduler-connection-broken
2022-12-12 15:26:57,188 - distributed.nanny - WARNING - Worker process still alive after 3.1999981689453127 seconds, killing
2022-12-12 15:26:57,189 - distributed.nanny - WARNING - Worker process still alive after 3.1999958801269535 seconds, killing
2022-12-12 15:26:57,478 - distributed.nanny - INFO - Worker process 761275 was killed by signal 9
2022-12-12 15:26:57,551 - distributed.nanny - INFO - Worker process 761278 was killed by signal 9
